{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG2EYc3CoU74"
      },
      "outputs": [],
      "source": [
        "video_path = '/content/ball.mp4'\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "!pip install supervision\n",
        "import supervision as sv\n",
        "\n",
        "byte_tracker = sv.ByteTrack()\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Define the codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "out = cv2.VideoWriter('output.mp4', fourcc, 30.0, (1280,  720))\n",
        "\n",
        "in_car = 0\n",
        "out_car = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Read the first frame\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to load the video.\")\n",
        "        exit()\n",
        "\n",
        "    # Select the bounding box\n",
        "    detection = cv2.selectROI(first_frame)\n",
        "\n",
        "    # # convert to Detections\n",
        "    # detections = sv.Detections.from_ultralytics(results)\n",
        "    # # only consider class id from selected_classes define above\n",
        "    # detections = detections[np.isin(detections.class_id, selected_classes)]\n",
        "\n",
        "    detections = byte_tracker.update_with_detections(detection)\n",
        "    frame = box_anotator(frame, detections)\n",
        "\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything if job is finished\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=pretrained)\n",
        "        # Extract layers up to conv4\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.feature_extractor(x)\n",
        "\n",
        "class Neck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Neck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        return self.relu(x)\n",
        "\n",
        "class SiamRPNHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super(SiamRPNHead, self).__init__()\n",
        "        self.cls_head = nn.Conv2d(in_channels, num_anchors * 2, kernel_size=1)\n",
        "        self.reg_head = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_output = self.cls_head(x)  # Classification output\n",
        "        reg_output = self.reg_head(x)  # Regression output\n",
        "        return cls_output, reg_output\n",
        "\n",
        "class SiamRPNPP(nn.Module):\n",
        "    def __init__(self, backbone, neck, head):\n",
        "        super(SiamRPNPP, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.neck = neck\n",
        "        self.head = head\n",
        "\n",
        "    def forward(self, template, search):\n",
        "        # Extract features from template and search images\n",
        "        template_feat = self.backbone(template)\n",
        "        search_feat = self.backbone(search)\n",
        "\n",
        "        # Pass features through neck\n",
        "        template_feat = self.neck(template_feat)\n",
        "        search_feat = self.neck(search_feat)\n",
        "\n",
        "        # Correlation (or cross-correlation)\n",
        "        corr_feat = self.cross_correlation(template_feat, search_feat)\n",
        "\n",
        "        # Pass through classification and regression heads\n",
        "        cls_output, reg_output = self.head(corr_feat)\n",
        "        return cls_output, reg_output\n",
        "\n",
        "    def cross_correlation(self, template, search):\n",
        "        # Perform depthwise cross-correlation\n",
        "        batch_size, c, h, w = search.size()\n",
        "        template = template.view(batch_size, c, -1).permute(0, 2, 1)\n",
        "        search = search.view(batch_size, c, -1)\n",
        "        corr = torch.matmul(template, search)  # Batch matrix multiplication\n",
        "        corr = corr.view(batch_size, 1, h, w)\n",
        "        return corr\n",
        "\n",
        "# Instantiate the components\n",
        "backbone = ResNetBackbone(pretrained=True)\n",
        "neck = Neck(in_channels=2048, out_channels=256)\n",
        "head = SiamRPNHead(in_channels=256, num_anchors=5)\n",
        "\n",
        "# Create the SiamRPN++ model\n",
        "model = SiamRPNPP(backbone, neck, head)\n",
        "\n",
        "# # Example usage\n",
        "# template = torch.randn(1, 3, 127, 127)  # Template image\n",
        "# search = torch.randn(1, 3, 255, 255)    # Search image\n",
        "# cls_output, reg_output = model(template, search)\n",
        "# print(\"Classification output shape:\", cls_output.shape)\n",
        "# print(\"Regression output shape:\", reg_output.shape)\n"
      ],
      "metadata": {
        "id": "Np7HsZIwpGMc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "template = torch.randn(1, 3, 127, 127)  # Template image\n",
        "search = torch.randn(1, 3, 255, 255)    # Search image\n",
        "cls_output, reg_output = model(template, search)\n",
        "print(\"Classification output shape:\", cls_output.shape)\n",
        "print(\"Regression output shape:\", reg_output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "f9tyD460dkZj",
        "outputId": "fb7fff46-fb3f-4126-85b5-8396ae51ca9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[1, 1, 8, 8]' is invalid for input of size 1024",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9735533140be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m127\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Template image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Search image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcls_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Regression output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-10841b21d2d2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, template, search)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Correlation (or cross-correlation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mcorr_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Pass through classification and regression heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-10841b21d2d2>\u001b[0m in \u001b[0;36mcross_correlation\u001b[0;34m(self, template, search)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0msearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch matrix multiplication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1, 8, 8]' is invalid for input of size 1024"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Backbone with ResNet\n",
        "class ResNetBackbone(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetBackbone, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)  # Load pretrained ResNet-50\n",
        "        self.features = nn.Sequential(*list(resnet.children())[:-2])  # Remove fully connected layer and avgpool\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "# Neck (e.g., Adjust Layer)\n",
        "class Neck(nn.Module):\n",
        "    def __init__(self, in_channels=2048, out_channels=256):\n",
        "        super(Neck, self).__init__()\n",
        "        self.adjust = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.adjust(x)\n",
        "\n",
        "# RPN Head\n",
        "class RPNHead(nn.Module):\n",
        "    def __init__(self, in_channels=256, anchor_num=5):\n",
        "        super(RPNHead, self).__init__()\n",
        "        self.cls = nn.Conv2d(in_channels, anchor_num * 2, kernel_size=1)  # Classification\n",
        "        self.reg = nn.Conv2d(in_channels, anchor_num * 4, kernel_size=1)  # Regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        cls_out = self.cls(x)\n",
        "        reg_out = self.reg(x)\n",
        "        return cls_out, reg_out\n",
        "\n",
        "# SiamRPN++ Model\n",
        "class SiamRPNPlusPlus(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiamRPNPlusPlus, self).__init__()\n",
        "        self.backbone = ResNetBackbone()\n",
        "        self.neck = Neck()\n",
        "        self.head = RPNHead()\n",
        "\n",
        "    def forward(self, z, x):\n",
        "        z_feat = self.backbone(z)\n",
        "        x_feat = self.backbone(x)\n",
        "\n",
        "        z_feat = self.neck(z_feat)\n",
        "        x_feat = self.neck(x_feat)\n",
        "\n",
        "        response = self.cross_correlation(z_feat, x_feat)\n",
        "        cls_out, reg_out = self.head(response)\n",
        "        return cls_out, reg_out\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_correlation(z, x):\n",
        "        N, C, H, W = x.size()\n",
        "        z = z.view(N, C, -1).permute(0, 2, 1)  # [N, H*W, C]\n",
        "        x = x.view(N, C, -1)  # [N, C, H*W]\n",
        "        response = torch.matmul(z, x).view(N, H, W, H, W)  # [N, H, W, H, W]\n",
        "        return response\n",
        "\n",
        "# Webcam-based Tracking Application\n",
        "import cv2\n",
        "\n",
        "def select_roi_and_track(model, device):\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Cannot open webcam.\")\n",
        "        return\n",
        "\n",
        "    _, frame = cap.read()\n",
        "    roi = cv2.selectROI(\"Select ROI\", frame, fromCenter=False)\n",
        "    x, y, w, h = map(int, roi)\n",
        "\n",
        "    template = frame[y:y+h, x:x+w]\n",
        "    template_tensor = preprocess(template).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        search_tensor = preprocess(frame).to(device)\n",
        "        with torch.no_grad():\n",
        "            cls_out, reg_out = model(template_tensor, search_tensor)\n",
        "\n",
        "        # Use cls_out and reg_out to calculate object position (simplified for this example)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.imshow(\"Tracking\", frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def preprocess(frame):\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.resize(frame, (255, 255))\n",
        "    frame = torch.tensor(frame).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
        "    return frame\n",
        "\n",
        "# Main execution\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = SiamRPNPlusPlus().to(device)\n",
        "\n",
        "    # Optionally load pretrained weights\n",
        "    pretrained_weights_path = \"siamrpnplusplus.pth\"\n",
        "    model.load_state_dict(torch.load(pretrained_weights_path, map_location=device))\n",
        "\n",
        "    select_roi_and_track(model, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "s_Q6cbNORpsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Open webcam\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Read the first frame\n",
        "ret, frame = cap.read()\n",
        "if not ret:\n",
        "    print(\"Failed to grab frame\")\n",
        "    exit()\n",
        "\n",
        "# Select ROI\n",
        "roi = cv2.selectROI(\"Select ROI\", frame, fromCenter=False, showCrosshair=True)\n",
        "cv2.destroyWindow(\"Select ROI\")\n",
        "\n",
        "# Crop the ROI as the template\n",
        "x, y, w, h = roi\n",
        "template = frame[y:y+h, x:x+w]\n",
        "\n",
        "# Resize the template to 127x127\n",
        "template = cv2.resize(template, (127, 127))\n"
      ],
      "metadata": {
        "id": "L4ZuHM6w1QlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "while True:\n",
        "    # Read a new frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Define a search region around the last known position\n",
        "    search_x, search_y = max(x - w, 0), max(y - h, 0)\n",
        "    search_w, search_h = 3 * w, 3 * h  # Enlarge the search area\n",
        "    search = frame[search_y:search_y+search_h, search_x:search_x+search_w]\n",
        "\n",
        "    # Resize the search region to 255x255\n",
        "    search = cv2.resize(search, (255, 255))\n",
        "\n",
        "    # Convert template and search images to tensors\n",
        "    template_tensor = F.to_tensor(template).unsqueeze(0).cuda()\n",
        "    search_tensor = F.to_tensor(search).unsqueeze(0).cuda()\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        cls_output, reg_output = model(template_tensor, search_tensor)\n",
        "\n",
        "    # Decode the model's outputs to get the new bounding box\n",
        "    # (You will need to implement the decoding logic based on your regression output)\n",
        "    x, y, w, h = decode_bbox(cls_output, reg_output)\n",
        "\n",
        "    # Update the bounding box and draw it on the frame\n",
        "    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow(\"Tracking\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "vVCiImrQ1S-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}